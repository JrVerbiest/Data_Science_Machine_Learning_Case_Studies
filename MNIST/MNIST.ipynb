{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - Hello World of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem statement:** \n",
    "<br><br>\n",
    "Classify grayscale image of handwritten digits (28 x 28 pixels) into 10 categories (0 through 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8ed54f3f90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8ed55a1910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28*28)\n",
    "train_images = train_images.astype('float')/255\n",
    "\n",
    "test_images = test_images.reshape(10000, 28*28)\n",
    "test_images = test_images.astype('float')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network: Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dense = Sequential()\n",
    "network_dense.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network_dense.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401920"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 401920\n",
    "# Input is 28x28\n",
    "# bias= 512\n",
    "28*28*512+512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Dense Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The compilation step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dense.compile(optimizer='rmsprop', \n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit (train dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.2589 - accuracy: 0.9243\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1049 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0692 - accuracy: 0.9801\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0500 - accuracy: 0.9847\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0378 - accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ed5d7c9d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_dense.fit(train_images, train_labels, epochs=5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Accurancy = 98.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate (test dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0677 - accuracy: 0.9793\n",
      "test_acc: 0.9793000221252441\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc = network_dense.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accurancy = 98.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network: convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28,1))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The network architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convnet\n",
    "model_convnet = models.Sequential()\n",
    "model_convnet.add(layers.Conv2D(32, (3,3), activation='relu', input_shape = (28,28,1)))\n",
    "model_convnet.add(layers.MaxPooling2D((2,2)))\n",
    "model_convnet.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model_convnet.add(layers.MaxPooling2D((2,2)))\n",
    "model_convnet.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
    "\n",
    "# add a dense layer\n",
    "model_convnet.add(layers.Flatten())\n",
    "model_convnet.add(layers.Dense(64, activation ='relu'))\n",
    "model_convnet.add(layers.Dense(10,activation ='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: input_shape = (28,28,1) => (image_height, image_width,image_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D(output_depth, (window_height, window_width)) <br><br>\n",
    "(window_height, window_width) =>typ. 3x3 (common choice) or 5x5 <br>\n",
    "output_depth => Depth of the output feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The compilation step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convnet.compile(optimizer='rmsprop', \n",
    "                      loss = 'categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit (train dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.1766 - accuracy: 0.9451\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0468 - accuracy: 0.9857\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 37s 39ms/step - loss: 0.0322 - accuracy: 0.9901\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0242 - accuracy: 0.9925\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 42s 45ms/step - loss: 0.0195 - accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ea2b63c90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_convnet.fit(train_images, train_labels, epochs=5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Accurancy = 99.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate (test dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0299 - accuracy: 0.9916\n",
      "test_acc: 0.991599977016449\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc = model_convnet.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accurancy = 99.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental difference between a densely connected layer and a convolution layer is this: \n",
    "- Dense layers learn global patterns in their input feature space \n",
    "- Convolution layers learn local patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EOF]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
